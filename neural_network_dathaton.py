# -*- coding: utf-8 -*-
"""neural_network_dathaton.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_Nn2xyj6ZOztpxuJLU2QISmX0ryB-b8f
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras import regularizers
import numpy as np 
import random

random.seed(42)
np.random.seed(42)
tf.random.set_seed(42)

df_train=pd.read_csv('train.csv')
df_new_train = pd.read_csv('new_train.csv')

df_new_test = pd.read_csv('new_test.csv')
df_new_test = df_new_test.sort_values('id')

UNWANTED_COL = [ 'Year_Factor','direction_max_wind_speed','direction_peak_wind_speed','max_wind_speed' ,'days_with_fog'  ,'id'  ]
for header in UNWANTED_COL:
  df_train.pop(header)

UNWANTED_COL = [ 'Year_Factor','direction_max_wind_speed','direction_peak_wind_speed','max_wind_speed' ,'days_with_fog'  ,'id'  ]
for header in UNWANTED_COL:
  df_new_train.pop(header)

UNWANTED_COL = [ 'Year_Factor','direction_max_wind_speed','direction_peak_wind_speed','max_wind_speed' ,'days_with_fog'  ,'id'  ]
for header in UNWANTED_COL:
  df_new_test.pop(header)

df={key: value[:,tf.newaxis] for key,value in df_new_test.items()}
ds_test= tf.data.Dataset.from_tensor_slices((dict(df)))

df_train = df_train.dropna()

#transformar los datos a formato tf.data.Dataset
def df_to_dataset(dataframe,shuffle=True,batch_size=32):
  df=dataframe.copy()
  labels=df.pop('site_eui')
  df={key: value[:,tf.newaxis] for key,value in df.items()}
  ds= tf.data.Dataset.from_tensor_slices((dict(df),labels))
  if shuffle==True:
    ds=ds.shuffle(buffer_size=len(dataframe))
  ds =ds.batch(batch_size)
  ds=ds.prefetch(batch_size)
  
  return ds

#creacion de las capas para normalizar datos numericos
def get_normalization_layer(Name,dataset):
  normalizer=tf.keras.layers.Normalization(axis=None)
  feature_ds=dataset.map(lambda x,y:x[Name])
  normalizer.adapt(feature_ds)

  return normalizer

#capas de la red neuronal para valores categoricos
def get_categorical_layer(Name,dataset,dtype,max_tokens=None):
  if dtype == 'string':
    index= tf.keras.layers.StringLookup(max_tokens=max_tokens)
  
  else:
    index = tf.keras.layers.IntegerLookup(max_tokens=max_tokens)

  feature_ds = dataset.map(lambda x, y: x[Name])
  index.adapt(feature_ds)

  encoder = tf.keras.layers.CategoryEncoding(num_tokens=index.vocabulary_size())
  
  return lambda feature: encoder(index(feature))

def get_embeding_layer(Name,dataset,max_tokens=None,vocabulary=None):
  if dtype == 'string':
    index= tf.keras.layers.StringLookup(max_tokens=max_tokens)
  
  else:
    index = tf.keras.layers.IntegerLookup(max_tokens=max_tokens)

  feature_ds = dataset.map(lambda x, y: x[Name])
  index.adapt(feature_ds)

  encoder = tf.keras.layers.CategoryEncoding(num_tokens=index.vocabulary_size())
  encoder=tf.keras.layers.Embedding(4,5)
  return lambda feature: encoder(index(feature))
  

#grafica de las curvas de validacion y entrenamiento  
def plot_loss_curves(history):
  """
  Returns separate loss curves for training and validation metrics.
  """ 
  loss = history.history['loss']
  val_loss = history.history['val_loss']

  accuracy = history.history['root_mean_squared_error']
  val_accuracy = history.history['val_root_mean_squared_error']

  epochs = range(len(history.history['loss']))

  # Plot loss
  plt.plot(epochs, loss, label='training_loss')
  plt.plot(epochs, val_loss, label='val_loss')
  plt.title('Loss')
  plt.xlabel('Epochs')
  plt.legend()

  # Plot accuracy
  plt.figure()
  plt.plot(epochs, accuracy, label='training_root_mean_squared_error')
  plt.plot(epochs, val_accuracy, label='val_root_mean_squared_error')
  plt.title('root_mean_squared_error')
  plt.xlabel('Epochs')
  plt.legend();

CATEGORICAL_FEATURES = [column for column in df_train.columns if df_train[column].dtype == 'O']
NUMERICAL_FEATURES = [column for column in df_train.columns if df_train[column].dtype != 'O']

NUMERICAL_FEATURES.pop()

df_train, df_val = train_test_split(df_train, test_size=0.30,random_state=42)

df_new_train, df_new_val = train_test_split(df_new_train, test_size=0.30,random_state=42)

train_ds=df_to_dataset(df_train)
val_ds=df_to_dataset(df_val,shuffle=False)

train_new_ds=df_to_dataset(df_new_train)
val_new_ds=df_to_dataset(df_new_val,shuffle=False)

all_inputs = []
encoded_features=[]

#creacion de las capas de entrada
for header in NUMERICAL_FEATURES:
  numeric_col=tf.keras.layers.Input(shape=(1,),name=header)
  normalization_layer=get_normalization_layer(header, train_ds)
  encoded_numeric_col= normalization_layer(numeric_col)
  all_inputs.append(numeric_col)
  encoded_features.append(encoded_numeric_col)

for header in CATEGORICAL_FEATURES:
  categorical_col=tf.keras.layers.Input(shape=(1,),name=header,dtype='string')
  encoding_layer=get_categorical_layer(Name=header,dataset=train_ds,dtype='string')
  encoded_cate_col=encoding_layer(categorical_col)
  all_inputs.append(categorical_col)
  encoded_features.append(encoded_cate_col)

all_inputs_2 = []
encoded_features_2=[]

for header in NUMERICAL_FEATURES:
  numeric_col=tf.keras.layers.Input(shape=(1,),name=header)
  normalization_layer=get_normalization_layer(header, train_new_ds)
  encoded_numeric_col= normalization_layer(numeric_col)
  all_inputs_2.append(numeric_col)
  encoded_features_2.append(encoded_numeric_col)

for header in CATEGORICAL_FEATURES:
  categorical_col=tf.keras.layers.Input(shape=(1,),name=header,dtype='string')
  encoding_layer=get_categorical_layer(Name=header,dataset=train_new_ds,dtype='string')
  encoded_cate_col=encoding_layer(categorical_col)
  all_inputs_2.append(categorical_col)
  encoded_features_2.append(encoded_cate_col)

checkpoint_filepath = '/tmp/checkpoint'
model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_weights_only=True,
    monitor='val_root_mean_squared_error',
    mode='min',
    save_best_only=True)

#creacion del modelo
all_features = tf.keras.layers.concatenate(encoded_features)
x = tf.keras.layers.Dense(64, activation="relu",kernel_regularizer=regularizers.l2(0.001))(all_features)
x = tf.keras.layers.Dropout(0.2)(x)
x = tf.keras.layers.Dense(128, activation="relu",kernel_regularizer=regularizers.l2(0.001))(x)
x = tf.keras.layers.Dropout(0.2)(x)
x = tf.keras.layers.Dense(128,activation='relu',kernel_regularizer=regularizers.l2(0.001))(x)
x = tf.keras.layers.Dropout(0.2)(x)
x = tf.keras.layers.Dense(128,activation='relu',kernel_regularizer=regularizers.l2(0.001))(x)
x = tf.keras.layers.Dropout(0.2)(x)
x = tf.keras.layers.Dense(32,activation='relu',kernel_regularizer=regularizers.l2(0.001))(x)

output = tf.keras.layers.Dense(1)(x)

model = tf.keras.Model(all_inputs, output)

N_VALIDATION = int(1e3)
N_TRAIN = int(1e4)
BUFFER_SIZE = int(1e4)
BATCH_SIZE = 32
STEPS_PER_EPOCH = N_TRAIN//BATCH_SIZE

lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(
  0.0005,
  decay_steps=STEPS_PER_EPOCH*1000,
  decay_rate=1,
  staircase=False)

def get_optimizer():
  return tf.keras.optimizers.RMSprop(lr_schedule)
optimizer = get_optimizer()

model.compile(
    optimizer=optimizer,
    loss='mse',
    metrics=[tf.keras.metrics.RootMeanSquaredError()])

history=model.fit(train_ds, epochs=100,validation_data=val_ds,callbacks=[model_checkpoint_callback])

df={key: value[:,tf.newaxis] for key,value in df_test.items()}
ds= tf.data.Dataset.from_tensor_slices((dict(df)))

predict = model.predict(ds_test)

#salvar modelo
model.save('/content/model_2')

sub = pd.read_csv("sample_solution.csv")
sub["site_eui"] = predict
sub.to_csv("submission_tensorflow_2.csv", index = False)

model.save('model_1')

!zip -r model_2.zip model_2/

from keras.models import load_model
new_model = load_model('model_2')

new_model.predict(ds_test)

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

#encontrar parametros optimos
from tensorboard.plugins.hparams import api as hp

HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([64, 128, 224]))
HP_NUM_UNITS_2 = hp.HParam('num_units 2', hp.Discrete([32,64,128]))
HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.4, 0.6))
HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam','RMSprop']))
HP_L2 = hp.HParam('l2 regularizer', hp.RealInterval(.001,.01))

with tf.summary.create_file_writer('logs/hparam_tuning').as_default():
  hp.hparams_config(
    hparams=[HP_NUM_UNITS,HP_NUM_UNITS_2, HP_DROPOUT, HP_OPTIMIZER,HP_L2],
    metrics=[hp.Metric('root_mean_squared_error', display_name='RMSE')],
  )





def train_test_model(hparams):
  all_features = tf.keras.layers.concatenate(encoded_features)
  x = tf.keras.layers.Dense(hparams[HP_NUM_UNITS], activation="relu",kernel_regularizer=regularizers.l2(hparams[HP_L2]))(all_features)
  x = tf.keras.layers.Dropout(hparams[HP_DROPOUT])(x)
  x = tf.keras.layers.Dense(hparams[HP_NUM_UNITS],activation='relu',kernel_regularizer=regularizers.l2(hparams[HP_L2]))(x)
  x = tf.keras.layers.Dropout(hparams[HP_DROPOUT])(x)
  x = tf.keras.layers.Dense(hparams[HP_NUM_UNITS],activation='relu',kernel_regularizer=regularizers.l2(hparams[HP_L2]))(x)
  x = tf.keras.layers.Dropout(hparams[HP_DROPOUT])(x)
  x = tf.keras.layers.Dense(hparams[HP_NUM_UNITS_2],activation='relu',kernel_regularizer=regularizers.l2(hparams[HP_L2]))(x)
  x = tf.keras.layers.Dropout(hparams[HP_DROPOUT])(x)
  x = tf.keras.layers.Dense(hparams[HP_NUM_UNITS_2],activation='relu',kernel_regularizer=regularizers.l2(hparams[HP_L2]))(x)

  output = tf.keras.layers.Dense(1)(x)

  model = tf.keras.Model(all_inputs, output)
 
  
  model.compile(
  optimizer=hparams[HP_OPTIMIZER],
  loss='mse',
  metrics=[tf.keras.metrics.RootMeanSquaredError()])
  model.fit(train_ds, epochs=30,validation_data=val_ds) # Run with 1 epoch to speed things up for demo purposes
  _,accuracy = model.evaluate(val_ds)
  return accuracy

def run(run_dir, hparams):
  with tf.summary.create_file_writer(run_dir).as_default():
    hp.hparams(hparams)  # record the values used in this trial
    accuracy = train_test_model(hparams)
    tf.summary.scalar('root_mean_squared_error', accuracy, step=1)

session_num = 0
for num_units1 in HP_NUM_UNITS.domain.values:
  for num_units2 in HP_NUM_UNITS_2.domain.values:
    for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):
      for l2 in (HP_L2.domain.min_value, HP_L2.domain.max_value):
        for optimizer in HP_OPTIMIZER.domain.values:
          
          hparams = {
              HP_NUM_UNITS: num_units1,
              HP_NUM_UNITS_2: num_units2,
              HP_DROPOUT: dropout_rate,
              HP_L2: l2,
              HP_OPTIMIZER: optimizer
              
          }
          run_name = "run-%d" % session_num
          print('--- Starting trial: %s' % run_name)
          print({h.name: hparams[h] for h in hparams})
          run('logs/hparam_tuning/' + run_name, hparams)
          session_num += 1

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir logs/hparam_tuning

from tensorboard.plugins.hparams import api as hp

HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([128, 224,448]))
HP_NUM_UNITS_2 = hp.HParam('num_units 2', hp.Discrete([128, 224,448]))
HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.2, 0.4))
HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['RMSprop']))
HP_L2 = hp.HParam('l2 regularizer', hp.RealInterval(.0001,0.001))
HP_NUM_CAP = hp.HParam('numero de capas',hp.Discrete([5,10,15]))

with tf.summary.create_file_writer('logs/hparam_tuning').as_default():
  hp.hparams_config(
    hparams=[HP_NUM_UNITS,HP_NUM_UNITS_2, HP_DROPOUT, HP_OPTIMIZER,HP_L2,HP_NUM_CAP],
    metrics=[hp.Metric('root_mean_squared_error', display_name='RMSE')],
  )

def train_test_model(hparams):
  all_features = tf.keras.layers.concatenate(encoded_features)
  x = tf.keras.layers.Dense(64, activation="relu",kernel_regularizer=regularizers.l2(hparams[HP_L2]))(all_features)
  x = tf.keras.layers.Dropout(.2)(x)
  
  for i in range(hparams[HP_NUM_CAP]):
    x = tf.keras.layers.Dense(hparams[HP_NUM_UNITS],activation='relu',kernel_regularizer=regularizers.l2(hparams[HP_L2]))(x)
    x = tf.keras.layers.Dropout(hparams[HP_DROPOUT])(x)

  for i in range(hparams[HP_NUM_CAP]):
    x = tf.keras.layers.Dense(hparams[HP_NUM_UNITS_2],activation='relu',kernel_regularizer=regularizers.l2(hparams[HP_L2]))(x)
    x = tf.keras.layers.Dropout(hparams[HP_DROPOUT])(x)
  
  x = tf.keras.layers.Dense(hparams[HP_NUM_UNITS],activation='relu',kernel_regularizer=regularizers.l2(hparams[HP_L2]))(x)
  x = tf.keras.layers.Dropout(.2)(x)
  x = tf.keras.layers.Dense(64,activation='relu',kernel_regularizer=regularizers.l2(hparams[HP_L2]))(x)

  output = tf.keras.layers.Dense(1)(x)

  model = tf.keras.Model(all_inputs, output)
 
  
  model.compile(
  optimizer=hparams[HP_OPTIMIZER],
  loss='mse',
  metrics=[tf.keras.metrics.RootMeanSquaredError()])
  model.fit(train_new_ds, epochs=50,validation_data=val_new_ds) # Run with 1 epoch to speed things up for demo purposes
  _,accuracy = model.evaluate(ds)
  return accuracy

def run(run_dir, hparams):
  with tf.summary.create_file_writer(run_dir).as_default():
    hp.hparams(hparams)  # record the values used in this trial
    accuracy = train_test_model(hparams)
    tf.summary.scalar('root_mean_squared_error', accuracy, step=1)

session_num = 0
for num_units1 in HP_NUM_UNITS.domain.values:
  for num_units2 in HP_NUM_UNITS_2.domain.values:
    for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):
      for l2 in (HP_L2.domain.min_value, HP_L2.domain.max_value):
        for optimizer in HP_OPTIMIZER.domain.values:
          for num_cap in HP_NUM_CAP.domain.values:

            hparams = {
                HP_NUM_UNITS: num_units1,
                HP_NUM_UNITS_2: num_units2,
                HP_DROPOUT: dropout_rate,
                HP_L2: l2,
                HP_OPTIMIZER: optimizer,
                HP_NUM_CAP: num_cap,
                
            }
          run_name = "run-%d" % session_num
          print('--- Starting trial: %s' % run_name)
          print({h.name: hparams[h] for h in hparams})
          run('logs/hparam_tuning/' + run_name, hparams)
          session_num += 1

from tensorboard.plugins.hparams import api as hp

HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([128, 224]))
HP_NUM_UNITS_2 = hp.HParam('num_units 2', hp.Discrete([64,128,224]))
HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.1, 0.3))
HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['RMSprop']))
HP_L2 = hp.HParam('l2 regularizer', hp.Discrete([.0001]))

with tf.summary.create_file_writer('logs/hparam_tuning').as_default():
  hp.hparams_config(
    hparams=[HP_NUM_UNITS,HP_NUM_UNITS_2, HP_DROPOUT, HP_OPTIMIZER,HP_L2],
    metrics=[hp.Metric('root_mean_squared_error', display_name='RMSE')],
  )

def train_test_model(hparams):
  all_features = tf.keras.layers.concatenate(encoded_features)
  x = tf.keras.layers.Dense(64, activation="relu",kernel_regularizer=regularizers.l2(.0001))(all_features)
  x = tf.keras.layers.Dropout(.2)(x)
  x = tf.keras.layers.Dense(64,activation='relu',kernel_regularizer=regularizers.l2(.0001))(x)
  x = tf.keras.layers.Dropout(.2)(x)
  x = tf.keras.layers.Dense(128,activation='relu',kernel_regularizer=regularizers.l2(.0001))(x)
  x = tf.keras.layers.Dropout(.2)(x)
  x = tf.keras.layers.Dense(128,activation='relu',kernel_regularizer=regularizers.l2(.0001))(x)
  x = tf.keras.layers.Dropout(.2)(x)
  x = tf.keras.layers.Dense(hparams[HP_NUM_UNITS],activation='relu',kernel_regularizer=regularizers.l2(.0001))(x)
  x = tf.keras.layers.Dropout(hparams[HP_DROPOUT])(x)
  x = tf.keras.layers.Dense(hparams[HP_NUM_UNITS_2],activation='relu',kernel_regularizer=regularizers.l2(.0001))(x)
  x = tf.keras.layers.Dropout(hparams[HP_DROPOUT])(x)
  x = tf.keras.layers.Dense(hparams[HP_NUM_UNITS_2],activation='relu',kernel_regularizer=regularizers.l2(.0001))(x)
  x = tf.keras.layers.Dropout(hparams[HP_DROPOUT])(x)


  output = tf.keras.layers.Dense(1)(x)

  model = tf.keras.Model(all_inputs, output)
 
  
  model.compile(
  optimizer=hparams[HP_OPTIMIZER],
  loss='mse',
  metrics=[tf.keras.metrics.RootMeanSquaredError()])
  model.fit(train_ds, epochs=45,validation_data=val_ds)
  _,accuracy = model.evaluate(val_ds)
  return accuracy

def run(run_dir, hparams):
  with tf.summary.create_file_writer(run_dir).as_default():
    hp.hparams(hparams)  # record the values used in this trial
    accuracy = train_test_model(hparams)
    tf.summary.scalar('root_mean_squared_error', accuracy, step=1)

session_num = 0
for num_units1 in HP_NUM_UNITS.domain.values:
  for num_units2 in HP_NUM_UNITS_2.domain.values:
    for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):
      for l2 in (HP_L2.domain.values):
        for optimizer in HP_OPTIMIZER.domain.values:
          
          hparams = {
              HP_NUM_UNITS: num_units1,
              HP_NUM_UNITS_2: num_units2,
              HP_DROPOUT: dropout_rate,
              HP_L2: l2,
              HP_OPTIMIZER: optimizer
              
          }
          run_name = "run-%d" % session_num
          print('--- Starting trial: %s' % run_name)
          print({h.name: hparams[h] for h in hparams})
          run('logs/hparam_tuning/' + run_name, hparams)
          session_num += 1

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir logs/hparam_tuning

import pandas as pd
from sklearn import preprocessing
import numpy as np
import lightgbm as lgb
import gc
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import KFold
from sklearn.model_selection import GroupKFold
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error

df_train=pd.read_csv('train.csv')
df_new_train = pd.read_csv('new_train.csv')

df_new_test = pd.read_csv('new_test.csv')
df_new_test = df_new_test.sort_values('id')

df={key: value[:,tf.newaxis] for key,value in df_new_test.items()}
ds_test= tf.data.Dataset.from_tensor_slices((dict(df)))

df_train=pd.read_csv('train.csv')

df_train = df_train.dropna()

UNWANTED_COL = ['direction_max_wind_speed','direction_peak_wind_speed','max_wind_speed' ,'days_with_fog'  ,'id'  ]
for header in UNWANTED_COL:
  df_train.pop(header)

no_usar=['Year_Factor','direction_max_wind_speed','direction_peak_wind_speed','max_wind_speed' ,'days_with_fog'  ,'id' ]

target='site_eui'

categorical=['Year_Factor','State_Factor','building_class','facility_type']

features=[x for x in df_train.columns if x not in no_usar]

CATEGORICAL_FEATURES = [column for column in df_train.columns if df_train[column].dtype == 'O']
NUMERICAL_FEATURES = [column for column in df_train.columns if df_train[column].dtype != 'O']

NUMERICAL_FEATURES.pop()

kfold=GroupKFold(n_splits=6)
pred_test=np.zeros(len(df_new_test))
fold_no = 1
for train_index,test_index in kfold.split(df_train,df_train[target],df_train['Year_Factor']):
  print(train_index)
  data_train = df_train.iloc[train_index]
  data_eval = df_train.iloc[test_index]

  train_ds=df_to_dataset(data_train)
  val_ds=df_to_dataset(data_eval,shuffle=False)

  all_inputs = []
  encoded_features=[]

  for header in NUMERICAL_FEATURES:
    numeric_col=tf.keras.layers.Input(shape=(1,),name=header)
    normalization_layer=get_normalization_layer(header, train_ds)
    encoded_numeric_col= normalization_layer(numeric_col)
    all_inputs.append(numeric_col)
    encoded_features.append(encoded_numeric_col)

  for header in CATEGORICAL_FEATURES:
    categorical_col=tf.keras.layers.Input(shape=(1,),name=header,dtype='string')
    encoding_layer=get_categorical_layer(Name=header,dataset=train_ds,dtype='string')
    encoded_cate_col=encoding_layer(categorical_col)
    all_inputs.append(categorical_col)
    encoded_features.append(encoded_cate_col)
  
  checkpoint_filepath = '/checkpoint'+'_'+'foldno'+str(fold_no)
  model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_weights_only=True,
    monitor='val_root_mean_squared_error',
    mode='min',
    save_best_only=True)

  all_features = tf.keras.layers.concatenate(encoded_features)
  x = tf.keras.layers.Dense(128, activation="relu",kernel_regularizer=regularizers.l2(0.001))(all_features)
  x = tf.keras.layers.Dropout(0.2)(x)
  x = tf.keras.layers.Dense(128, activation="relu",kernel_regularizer=regularizers.l2(0.001))(x)
  x = tf.keras.layers.Dropout(0.2)(x)
  x = tf.keras.layers.Dense(128,activation='relu',kernel_regularizer=regularizers.l2(0.001))(x)
  x = tf.keras.layers.Dropout(0.2)(x)
  x = tf.keras.layers.Dense(128,activation='relu',kernel_regularizer=regularizers.l2(0.001))(x)
  x = tf.keras.layers.Dropout(0.2)(x)
  x = tf.keras.layers.Dense(32,activation='relu',kernel_regularizer=regularizers.l2(0.001))(x)

  output = tf.keras.layers.Dense(1)(x)

  model = tf.keras.Model(all_inputs, output)

  N_VALIDATION = int(1e3)
  N_TRAIN = int(1e4)
  BUFFER_SIZE = int(1e4)
  BATCH_SIZE = 32
  STEPS_PER_EPOCH = N_TRAIN//BATCH_SIZE

  lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(
    0.0005,
    decay_steps=STEPS_PER_EPOCH*1000,
    decay_rate=1,
    staircase=False)

  def get_optimizer():
    return tf.keras.optimizers.Adam(lr_schedule)
  optimizer = get_optimizer()

  model.compile(
      optimizer=optimizer,
      loss='mse',
      metrics=[tf.keras.metrics.RootMeanSquaredError()])
  
  print('------------------------------------------------------------------------')
  print(f'Training for fold {fold_no} ...')

  history=model.fit(train_ds, epochs=300,validation_data=val_ds,callbacks=[model_checkpoint_callback])
  model.load_weights(checkpoint_filepath)

  pred_test=pred_test+model.predict(ds_test)
  fold_no = fold_no + 1

pred_test=(pred_test/6)
dict_resultados={}
dict_resultados['predicciones']=pred_test

